[input]
csvFile = ../Data/trainOnThis.csv
header = 0  
#0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62
dateColumn = None    
columns = 0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62
look_back = 1
y_column = 32
winLength = 18
trainTestSplit = 0.2
smoothingParam = 6
    
## reference value for normalisation
refvalue = 12000.0

datetopred = 2017-05-30

[switches]
verbosity = 1
## train on windows? 
windowedData = on
split = on
## normalise and partition data, options are: 
## make windows and normalise windows (on ref. value) = 3
normalise = 0
smoothingSwitch = off

[analysis]
## If plotting of true vs predicted data is wanted
plotting = on
## you want some eval metrics? go get them big boy!
evalMetrics = on

[network]
## prediction of time series: timeDistributed=on ; single point in time: timeDistributed=off
timeDistributed=off
attention=off
doubleattention=on
cnn = off
bidirect = off
batchnorm = off
inputDim = 33
earlyStop = 0.001
# prediction of single point in time
# prediction of time series
neuronsPerLayer = 64,64
outputLength = 1
outputDim = 1
## if you choose selu as activation function dont use batchnormalisation
activationPerLayer = selu
recurrentActivation = hard_sigmoid
initWeights = uniform
dropout = 0.3
optimiser = adam
learningRate = 0.001
decay = 0.1
loss = mse
epochs = 500
batchSize = 8
## Tensorflow loglevel, 0 = all, 1 = no info, 2 = no warning, 3 = no error
loglevel = 2
## batch normalisation

[tuning]
## if hyperparamter tuning should be done or not
tuning = off
## number of layers that should be tested
nlayer_tune = 4
## activation functions that should be tested
actlayer_tune = relu,elu,tanh,sigmoid
## recurrent activation tune
recactlayer_tune = hard_sigmoid
## number of hidden units that should be tested
nhiduplayer_tune = 50,100,150,200
## which dropout parameters should be tested
dropout_tune = 0.1,0.15,0.2,0.3
## learning rates
## lr_tune = 0.0001,0.001,0.01,0.1
lr_tune = 0.0001,0.0001
## batchsizes
batchsize_tune = 256,512,1024
## batch normalisation
batchnorm_tune = on,off


[output]
predictionFile = ../Data/predictions.csv
bestParams = ../Data/tuned.params
jsonFile = ../Data/normTest.json
modelFile = ../Data/normTest.h5

