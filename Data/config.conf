[input]
csvFile = ../Data/hourly.csv
header = 0 
dateColumn = None    
columns = 0,1,2
look_back = 1
y_column = 2
winLength = 27
trainTestSplit = 0.2
smoothingParam = 6
    
## reference value for normalisation
refvalue = 12000.0

datetopred = 2017-05-30

[switches]
verbosity = 1
## train on windows? 
windowedData = on
split = on
## normalise and partition data, options are: 
## make windows and normalise windows (on ref. value) = 3
normalise = 4
smoothingSwitch = off

[analysis]
## If plotting of true vs predicted data is wanted
plotting = on
## you want some eval metrics? go get them big boy!
evalMetrics = on

[network]
## prediction of time series: timeDistributed=on ; single point in time: timeDistributed=off
timeDistributed=off
cnn = off
bidirect = off
inputDim = 3
earlyStop = 0.01
# prediction of single point in time
# prediction of time series
neuronsPerLayer = 128,128
outputLength = 1
activationPerLayer = sigmoid
recurrentActivation = hard_sigmoid
initWeights = uniform
dropout = 0.3
optimiser = adam
learningRate = 0.0001
decay = 1e-5
loss = mse
epochs = 50
batchSize = 512
## Tensorflow loglevel, 0 = all, 1 = no info, 2 = no warning, 3 = no error
loglevel = 2
## batch normalisation
batchnorm = on

[tuning]
## if hyperparamter tuning should be done or not
tuning = off
## number of layers that should be tested
nlayer_tune = 4
## activation functions that should be tested
actlayer_tune = relu,elu,tanh,sigmoid
## recurrent activation tune
recactlayer_tune = hard_sigmoid
## number of hidden units that should be tested
nhiduplayer_tune = 50,100,150,200
## which dropout parameters should be tested
dropout_tune = 0.1,0.15,0.2,0.3
## learning rates
## lr_tune = 0.0001,0.001,0.01,0.1
lr_tune = 0.0001,0.0001
## batchsizes
batchsize_tune = 256,512,1024
## batch normalisation
batchnorm_tune = on,off


[output]
predictionFile = ../Data/predictions.csv
bestParams = ../Data/tuned.params
jsonFile = ../Data/normTest.json
modelFile = ../Data/normTest.h5

